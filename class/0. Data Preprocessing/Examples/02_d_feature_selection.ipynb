{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Feature Selection With scikit-learn**\n",
    "\n",
    "We are going to see a couple of methods to do feature selecction in scikit-learn. Take into account that is not always necesary to apply it to the dataset, and it will depend on the specific task and problem that we are working on. Some algorithms will have problems dealing with a high number of features, while others will be able to work with them. The **Curse of Dimensionality** is different for each algorithm.\n",
    "\n",
    "We are only going to see a few ot the available methods, a more in-depth discussion of all the feature selection methods available in scikit-lear can be found here: https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n"
   ],
   "metadata": {
    "id": "7gBhxuACclUa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Variance Threshold Feature Selection**\n",
    "\n",
    "A feature with a low variance means that it has a lot of similar values. Features with have mostly the same values are usually not very useful to discriminate the different clases. E.g. if almost everybody in this class has the same age, it would not be a good predictor of academic success."
   ],
   "metadata": {
    "id": "gXWQhc1Mh33F"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G4HW0KUBcW2h",
    "outputId": "78c30c18-8677-4e44-969b-ff5d3b73a6ab",
    "ExecuteTime": {
     "end_time": "2024-09-23T17:12:13.429270Z",
     "start_time": "2024-09-23T17:12:13.418996Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "# We are using another library that also has toy datasets https://seaborn.pydata.org/\n",
    "import seaborn as sns\n",
    "\n",
    "# We are only using the numerical attributes in the example\n",
    "mpg = sns.load_dataset('mpg').select_dtypes('number')\n",
    "mpg.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  model_year\n",
       "0  18.0          8         307.0       130.0    3504          12.0          70\n",
       "1  15.0          8         350.0       165.0    3693          11.5          70\n",
       "2  18.0          8         318.0       150.0    3436          11.0          70\n",
       "3  16.0          8         304.0       150.0    3433          12.0          70\n",
       "4  17.0          8         302.0       140.0    3449          10.5          70"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we are going to standarize the values before applying the feature selection:"
   ],
   "metadata": {
    "id": "z2zECPoDfsfS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "mpg = pd.DataFrame(scaler.fit_transform(mpg), columns = mpg.columns)\n",
    "mpg.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "l93hL3V2fw2l",
    "outputId": "d0261e2b-efa8-4e1e-eca5-149941ef05d3",
    "ExecuteTime": {
     "end_time": "2024-09-23T17:12:26.819026Z",
     "start_time": "2024-09-23T17:12:26.810587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        mpg  cylinders  displacement  horsepower    weight  acceleration  \\\n",
       "0 -0.706439   1.498191      1.090604    0.664133  0.630870     -1.295498   \n",
       "1 -1.090751   1.498191      1.503514    1.574594  0.854333     -1.477038   \n",
       "2 -0.706439   1.498191      1.196232    1.184397  0.550470     -1.658577   \n",
       "3 -0.962647   1.498191      1.061796    1.184397  0.546923     -1.295498   \n",
       "4 -0.834543   1.498191      1.042591    0.924265  0.565841     -1.840117   \n",
       "\n",
       "   model_year  \n",
       "0   -1.627426  \n",
       "1   -1.627426  \n",
       "2   -1.627426  \n",
       "3   -1.627426  \n",
       "4   -1.627426  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.706439</td>\n",
       "      <td>1.498191</td>\n",
       "      <td>1.090604</td>\n",
       "      <td>0.664133</td>\n",
       "      <td>0.630870</td>\n",
       "      <td>-1.295498</td>\n",
       "      <td>-1.627426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.090751</td>\n",
       "      <td>1.498191</td>\n",
       "      <td>1.503514</td>\n",
       "      <td>1.574594</td>\n",
       "      <td>0.854333</td>\n",
       "      <td>-1.477038</td>\n",
       "      <td>-1.627426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.706439</td>\n",
       "      <td>1.498191</td>\n",
       "      <td>1.196232</td>\n",
       "      <td>1.184397</td>\n",
       "      <td>0.550470</td>\n",
       "      <td>-1.658577</td>\n",
       "      <td>-1.627426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.962647</td>\n",
       "      <td>1.498191</td>\n",
       "      <td>1.061796</td>\n",
       "      <td>1.184397</td>\n",
       "      <td>0.546923</td>\n",
       "      <td>-1.295498</td>\n",
       "      <td>-1.627426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.834543</td>\n",
       "      <td>1.498191</td>\n",
       "      <td>1.042591</td>\n",
       "      <td>0.924265</td>\n",
       "      <td>0.565841</td>\n",
       "      <td>-1.840117</td>\n",
       "      <td>-1.627426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want a variance of 1, we would only get the weight column:"
   ],
   "metadata": {
    "id": "Zq3NvNffgECm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "selector = VarianceThreshold(1)\n",
    "selector.fit(mpg)\n",
    "mpg.columns[selector.get_support()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbVcKF7JgENK",
    "outputId": "7bdece7f-6e73-49e4-976d-4cc7d47b2c44",
    "ExecuteTime": {
     "end_time": "2024-09-23T17:13:48.449960Z",
     "start_time": "2024-09-23T17:13:48.445206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['weight'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method is more useful for **unsupervised learning**, were we don't have a class label.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "eXmoL7vNhIoD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Univariate Feature Selection with SelectKBest**\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. We can use different tests to do so: chi2, Pearson-correlation, etc...\n",
    "\n",
    "Let's start loading the dataset:"
   ],
   "metadata": {
    "id": "e5uDtPplhtZX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mpg = sns.load_dataset('mpg')\n",
    "# Divide the features into Independent and Dependent Variables.\n",
    "# If you don't remember what this means: https://www.pluralsight.com/guides/importing-and-splitting-data-into-dependent-and-independent-features-for-ml\n",
    "mpg = mpg.select_dtypes('number').dropna()\n",
    "X = mpg.drop('mpg' , axis =1)\n",
    "y = mpg['mpg']"
   ],
   "metadata": {
    "id": "OxNSUDu7kQE8",
    "ExecuteTime": {
     "end_time": "2024-09-23T17:57:28.468038Z",
     "start_time": "2024-09-23T17:57:28.461686Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we apply the feature selecction. We are going to use mutual_info_regression and select the top 2 variables:"
   ],
   "metadata": {
    "id": "VCj5OL1ClLOA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "selector = SelectKBest(mutual_info_regression, k=2)\n",
    "selector.fit(X, y)\n",
    "X.columns[selector.get_support()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0VD6R_tlN4s",
    "outputId": "1cb46763-6cea-4717-b4e5-f76d772948b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['displacement', 'weight'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "cpZWDrWBny3k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Recursive Feature Elimination (RFE)**\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. What this means is that we use a machine learning model to select the features by eliminating the least important feature after recursively training.\n",
    "\n",
    "First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "As always, first we load the dataset:"
   ],
   "metadata": {
    "id": "BgSNnjGnmWHC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "titanic = sns.load_dataset('titanic')[['survived', 'pclass', 'age', 'parch', 'sibsp', 'fare']].dropna()\n",
    "X = titanic.drop('survived', axis = 1)\n",
    "y = titanic['survived']"
   ],
   "metadata": {
    "id": "SuwuXb4OnSXW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we apply the feature selection. In this example we are using our old friend, the logistic regression:"
   ],
   "metadata": {
    "id": "h81GdFnbndUA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe_selector = RFE(estimator=LogisticRegression(),n_features_to_select = 2, step = 1)\n",
    "rfe_selector.fit(X, y)\n",
    "X.columns[rfe_selector.get_support()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iK7tw4dhnf4z",
    "outputId": "6ebe3878-ecd7-41a1-d63b-5b5db26306d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['pclass', 'parch'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "rGlfAbZIoC1F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature Selection via SelectFromModel**\n",
    "\n",
    "This method is similar to the previous one, but the feature selection is done using some importance metric. It is often  coef_ or feature_importances_ but it could be any callable. By default, the threshold is the mean.\n",
    "\n",
    "Using the previous example:"
   ],
   "metadata": {
    "id": "1w_fXEiDoGBv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm_selector = SelectFromModel(estimator=LogisticRegression())\n",
    "sfm_selector.fit(X, y)\n",
    "X.columns[sfm_selector.get_support()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6l7Vc9LPowqp",
    "outputId": "cd6d6e2a-6918-4e0d-e13f-780bd4076730"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['pclass'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "YtXtCH3Kq3BV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature Selection Sequential Feature Selection (SFS)**\n",
    "\n",
    "Sequential Feature Selection (SFS) is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n",
    "\n",
    "Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero features and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\n",
    "\n",
    "Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no features and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used."
   ],
   "metadata": {
    "id": "LHcgtUCYschj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs_selector = SequentialFeatureSelector(estimator=LogisticRegression(), n_features_to_select = 3, cv =10, direction ='backward')\n",
    "sfs_selector.fit(X, y)\n",
    "X.columns[sfs_selector.get_support()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPIdXs62sqT0",
    "outputId": "9ce2444a-e269-48ea-87f1-fbd6ab01ee06"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['pclass', 'age', 'parch'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "ZV4VkqY-svJM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Exercises**\n",
    "\n",
    "\n",
    "1.   Apply the Variance Threshold Feature Selection to any dataset included in scikit-learn.\n",
    "2.   Apply SelectKBest to any dataset included in scikit-learn. Use at least 3 diffirent statistical tests.\n",
    "3. Apply Recursive Feature Elimination (RFE) to any dataset included in scikit-learn. Use 2 other algorithms that are not the logistic regression.\n",
    "4. Apply SelectFromModel to any dataset included in scikit-learn. Use other importance metric that is not the default mean.\n",
    "\n",
    "While doing the exercises compare the results obtained with each method\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "5zbOFFtyq32n"
   }
  }
 ]
}
